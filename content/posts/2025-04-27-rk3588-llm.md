---
title: 瑞芯微 RK3588 DeepSeek LLM 体验
author: ffutop
date: 2025-04-27
categories:
    - 硬件
tags:
    - RK3588
    - LLM
    - DeepSeek
    - RKLLM
---

## RKLLM 简介

RKLLM 是 Rockchip 提供的软件栈，旨在帮助用户快速将大语言模型（LLM）部署到 Rockchip 芯片上。其整体框架如下：

<center class="half">
    <div style="background-color:#ffffff;">
    <img src="res/framework.jpg" title="RKLLM Framework"/>
</center>
<small>图1: RKLLM 框架</small>

RKLLM 主要包含以下组件：

1.  **RKLLM-Toolkit**: 运行在 PC 上的软件开发套件，用于模型转换和量化。
2.  **RKLLM Runtime**: 提供 C/C++ 编程接口，用于在 Rockchip NPU 平台上部署 RKLLM 模型，加速 LLM 应用的实现。
3.  **RKNPU Kernel Driver**: 负责与 NPU 硬件交互的内核驱动程序，已开源。

## 支持平台

*   RK3588 系列
*   RK3576 系列
*   RK3562 系列

## 支持模型

RKLLM 支持多种主流 LLM，包括：

*   LLAMA / TinyLLAMA
*   Qwen / Qwen2
*   Phi
*   ChatGLM3-6B
*   Gemma2 / Gemma3
*   InternLM2
*   MiniCPM
*   TeleChat
*   **DeepSeek-R1-Distill** / **Janus-Pro-1B**
*   以及 Qwen2-VL, MiniCPM-V, InternVL2 等多模态模型

*完整的支持列表和模型下载请参考官方文档或 [rkllm_model_zoo](https://console.box.lenovo.com/l/l0tXb8) (提取码: rkllm)*

## RK3588 性能基准 (部分)

下表展示了部分模型在 RK3588 上的性能表现：

| llm model      | platform | dtype     | seqlen | max_context | new_tokens | TTFT(ms) | Tokens/s | memory(G) |
| :------------- | :------: | :-------- | :----: | :---------: | :--------: | :------: | :------: | :-------: |
| Qwen2-0.5B     |  RK3588  | w8a8      |   64   |     320     |    256     |    79    |  41.50   |   0.62    |
| TinyLLAMA-1.1B |  RK3588  | w8a8      |   64   |     320     |    256     |   140    |  24.21   |   1.25    |
| Qwen2-1.5B     |  RK3588  | w8a8      |   64   |     320     |    256     |   206    |  16.46   |   2.47    |
| Phi-3-3.8B     |  RK3588  | w8a8      |   64   |     320     |    256     |   516    |   7.44   |   3.88    |
| ChatGLM3-6B    |  RK3588  | w8a8      |   64   |     320     |    256     |   800    |   4.95   |   6.69    |
| Gemma2-2B      |  RK3588  | w8a8      |   64   |     320     |    256     |   342    |   9.67   |   4.84    |
| InternLM2-1.8B |  RK3588  | w8a8      |   64   |     320     |    256     |   206    |  15.66   |   2.38    |
| MiniCPM3-4B    |  RK3588  | w8a8      |   64   |     320     |    256     |   702    |   6.15   |   4.65    |
| llama3-8B      |  RK3588  | w8a8      |   64   |     320     |    256     |   1128   |   3.79   |   9.21    |

*数据来源：[RKLLM GitHub](https://github.com/airockchip/rknn-llm)*
*性能测试基于各平台最大 CPU 和 NPU 频率。*
*DeepSeek-R1-Distill-Qwen-1.5B 模型性能可参考 Qwen2-1.5B。*

## DeepSeek 模型转换 (以 DeepSeek-R1-Distill-Qwen-1.5B 为例)

使用 RKLLM-Toolkit 将 Hugging Face 上的 DeepSeek 模型转换为 RKLLM 格式。

**前提:**

*   安装 `rkllm-toolkit==1.2.0` 或更高版本。
*   下载 [DeepSeek-R1-Distill-Qwen-1.5B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B) 模型文件。

**步骤:**

1.  **准备量化校准数据:**
    使用 fp16 模型的生成结果作为量化校准数据。运行脚本生成 `data_quant.json`。
2.  **导出 RKLLM 模型:**
    运行导出脚本，将模型转换为 `.rkllm` 格式。

```bash
# 假设模型文件位于 /path/to/DeepSeek-R1-Distill-Qwen-1.5B
# 进入 RKLLM SDK 的示例目录
cd rknn-llm/examples/DeepSeek-R1-Distill-Qwen-1.5B_Demo/export

# 生成量化数据
python generate_data_quant.py -m /path/to/DeepSeek-R1-Distill-Qwen-1.5B

# 导出 RKLLM 模型 (例如 W8A8 量化)
python export_rkllm.py --model_path /path/to/DeepSeek-R1-Distill-Qwen-1.5B --target_platform rk3588 --dtype w8a8 --output_path ./DeepSeek-R1-Distill-Qwen-1.5B_W8A8_RK3588.rkllm
```

*或者，您可以直接从 [rkllm_model_zoo](https://console.box.lenovo.com/l/l0tXb8) (提取码: rkllm) 下载转换好的模型。*

## DeepSeek C++ Demo 部署与运行

RKLLM SDK 提供了 C++ 示例代码用于在开发板上进行推理。

### 1. 编译 Demo

在 PC 上使用交叉编译工具链编译 Demo。

```bash
# 进入 C++ Demo 目录
cd rknn-llm/examples/DeepSeek-R1-Distill-Qwen-1.5B_Demo/deploy

# 设置交叉编译工具链路径 (根据实际情况修改)
export GCC_COMPILER=/path/to/your/aarch64-linux-gnu

# 执行编译脚本 (适用于 Linux)
./build-linux.sh
```

编译成功后，会在 `deploy/install/demo_Linux_aarch64` 目录下生成可执行文件 `llm_demo` 和 `lib` 文件夹。

### 2. 推送文件到 RK3588 开发板

使用 `adb` 或其他方式将编译产物和模型文件推送到开发板。

```bash
# 推送整个 install 目录
adb push install/demo_Linux_aarch64 /data/

# 推送转换好的 RKLLM 模型文件
adb push export/DeepSeek-R1-Distill-Qwen-1.5B_W8A8_RK3588.rkllm /data/demo_Linux_aarch64/

# 推送 RK3588 的频率固定脚本
adb push ../../../scripts/fix_freq_rk3588.sh /data/demo_Linux_aarch64/
```

### 3. 在 RK3588 开发板上运行 Demo

通过 `adb shell` 或串口连接到开发板终端。

```bash
# 进入 Demo 目录
cd /data/demo_Linux_aarch64

# 添加库文件路径到环境变量
export LD_LIBRARY_PATH=./lib:$LD_LIBRARY_PATH

# 执行频率固定脚本 (确保 NPU 和 CPU 运行在最高频率以获得最佳性能)
sh fix_freq_rk3588.sh

# 设置日志级别以查看性能信息 (可选)
export RKLLM_LOG_LEVEL=1

# 运行 Demo，指定模型路径、输入序列长度和最大上下文长度
./llm_demo DeepSeek-R1-Distill-Qwen-1.5B_W8A8_RK3588.rkllm 2048 4096
```

### 4. 运行示例

程序启动后会提示输入问题序号或自定义输入。

```
rkllm init start
rkllm init success

**********************可输入以下问题对应序号获取回答/或自定义输入********************

[0] 现有一笼子，里面有鸡和兔子若干只，数一数，共有头14个，腿38条，求鸡和兔子各有多少只？
[1] 有28位小朋友排成一行,从左边开始数第10位是学豆,从右边开始数他是第几位?

*************************************************************************

user: 0
现有一笼子，里面有鸡和兔子若干只，数一数，共有头14个，腿38条，求鸡和兔子各有多少只？
robot: <think>
首先，设鸡的数量为x，兔子的数量为y。
... (模型思考过程) ...
</think>

要解决这个问题，我们可以设鸡的数量为 \( x \)，兔子的数量为 \( y \)。根据题目给出的条件：
... (模型回答) ...
因此，鸡的数量是 **9只**，兔子的数量是 **5只**。

**最终答案：**
鸡有 \(\boxed{9}\) 只，兔子有 \(\boxed{5}\) 只。

user:
```

## 相关链接

*   **RKLLM SDK:** [https://github.com/airockchip/rknn-llm](https://github.com/airockchip/rknn-llm)
*   **RKLLM SDK 下载:** [RKLLM_SDK](https://console.zbox.filez.com/l/RJJDmB) (提取码: rkllm)
*   **RKLLM 模型库:** [rkllm_model_zoo](https://console.box.lenovo.com/l/l0tXb8) (提取码: rkllm)
*   **DeepSeek Demo:** [DeepSeek-R1-Distill-Qwen-1.5B_Demo](https://github.com/airockchip/rknn-llm/tree/main/examples/DeepSeek-R1-Distill-Qwen-1.5B_Demo)
*   **RKNN Toolkit2 (用于其他 AI 模型):** [https://github.com/airockchip/rknn-toolkit2](https://github.com/airockchip/rknn-toolkit2)

